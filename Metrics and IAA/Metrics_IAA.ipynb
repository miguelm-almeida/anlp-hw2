{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Csx5xCSwPp-X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from scipy.spatial.distance import cosine\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the following cell to get the metrics\n",
        "##### (Make sure that all the data files are present as required.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "72S_e7pyuHuA",
        "outputId": "8e39813f-3541-4f06-e9c6-d20c4f22a168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating QA metrics...\n",
            "\n",
            "=== QA Evaluation Results ===\n",
            "Exact Match: 0.0080\n",
            "F1 Score: 0.2300\n",
            "Semantic Similarity: 0.4451\n",
            "Answer Recall: 0.2669\n",
            "Evaluating QA metrics...\n",
            "\n",
            "=== QA Evaluation Results ===\n",
            "Exact Match: 0.0140\n",
            "F1 Score: 0.2389\n",
            "Semantic Similarity: 0.3974\n",
            "Answer Recall: 0.2214\n",
            "Evaluating QA metrics...\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text by removing punctuation, lowercasing, and removing stopwords.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def compute_exact_match(pred_answers, true_answers):\n",
        "    \"\"\"Compute exact match score between predicted and true answers.\"\"\"\n",
        "    matches = [pred.strip() == true.strip() for pred, true in zip(pred_answers, true_answers)]\n",
        "    return sum(matches) / len(matches) if matches else 0\n",
        "\n",
        "def compute_f1_score(pred_answers, true_answers):\n",
        "    \"\"\"Compute word-level F1 score between predicted and true answers.\"\"\"\n",
        "    f1_scores = []\n",
        "\n",
        "    for pred, true in zip(pred_answers, true_answers):\n",
        "        # Tokenize and create sets of words\n",
        "        pred_tokens = set(word_tokenize(preprocess_text(pred)))\n",
        "        true_tokens = set(word_tokenize(preprocess_text(true)))\n",
        "\n",
        "        # Skip empty answers\n",
        "        if not true_tokens or not pred_tokens:\n",
        "            continue\n",
        "\n",
        "        # Calculate precision, recall, F1\n",
        "        common_tokens = pred_tokens.intersection(true_tokens)\n",
        "        precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0\n",
        "        recall = len(common_tokens) / len(true_tokens) if true_tokens else 0\n",
        "\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    return sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
        "\n",
        "def compute_similarity_score(pred_answers, true_answers):\n",
        "    \"\"\"Compute semantic similarity between predicted and true answers using sentence embeddings.\"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    similarities = []\n",
        "    for pred, true in zip(pred_answers, true_answers):\n",
        "        # Skip empty answers\n",
        "        if not pred.strip() or not true.strip():\n",
        "            continue\n",
        "\n",
        "        # Get embeddings\n",
        "        pred_embedding = model.encode([pred])[0]\n",
        "        true_embedding = model.encode([true])[0]\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity = 1 - cosine(pred_embedding, true_embedding)\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    return sum(similarities) / len(similarities) if similarities else 0\n",
        "\n",
        "def compute_answer_recall(pred_answers, true_answers):\n",
        "    \"\"\"Compute the proportion of true answer words that appear in the predicted answer.\"\"\"\n",
        "    recall_scores = []\n",
        "\n",
        "    for pred, true in zip(pred_answers, true_answers):\n",
        "        # Tokenize\n",
        "        pred_tokens = set(word_tokenize(preprocess_text(pred)))\n",
        "        true_tokens = set(word_tokenize(preprocess_text(true)))\n",
        "\n",
        "        # Skip empty true answers\n",
        "        if not true_tokens:\n",
        "            continue\n",
        "\n",
        "        # Calculate recall\n",
        "        common_tokens = pred_tokens.intersection(true_tokens)\n",
        "        recall = len(common_tokens) / len(true_tokens)\n",
        "        recall_scores.append(recall)\n",
        "\n",
        "    return sum(recall_scores) / len(recall_scores) if recall_scores else 0\n",
        "\n",
        "def evaluate_qa_metrics(csv_path):\n",
        "    \"\"\"\n",
        "    Evaluate QA performance using multiple metrics:\n",
        "    - Exact Match\n",
        "    - F1 Score\n",
        "    - Semantic Similarity\n",
        "    - Answer Recall\n",
        "\n",
        "    Assumes the CSV has columns: 'answer' (model predictions) and 'correct_answer' (ground truth)\n",
        "    \"\"\"\n",
        "    # Read the merged CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    required_cols = ['output', 'correct_answer']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise ValueError(f\"CSV must contain these columns: {required_cols}\")\n",
        "\n",
        "    # Get predicted and true answers\n",
        "    pred_answers = df['output'].astype(str).tolist()\n",
        "    true_answers = df['correct_answer'].astype(str).tolist()\n",
        "\n",
        "    # Compute metrics\n",
        "    exact_match = compute_exact_match(pred_answers, true_answers)\n",
        "    f1 = compute_f1_score(pred_answers, true_answers)\n",
        "    similarity = compute_similarity_score(pred_answers, true_answers)\n",
        "    recall = compute_answer_recall(pred_answers, true_answers)\n",
        "\n",
        "    # Create results dictionary\n",
        "    results = {\n",
        "        'exact_match': exact_match,\n",
        "        'f1_score': f1,\n",
        "        'similarity_score': similarity,\n",
        "        'answer_recall': recall\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    csv_path = 'merged_qa_flant5.csv'  # Path to your merged QA file\n",
        "\n",
        "    print(\"Evaluating QA metrics...\")\n",
        "    metrics = evaluate_qa_metrics(csv_path)\n",
        "\n",
        "    print(\"\\n=== QA Evaluation Results for FLAN-T5 ===\")\n",
        "    print(f\"Exact Match: {metrics['exact_match']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
        "    print(f\"Semantic Similarity: {metrics['similarity_score']:.4f}\")\n",
        "    print(f\"Answer Recall: {metrics['answer_recall']:.4f}\")\n",
        "\n",
        "    csv_path = 'merged_qa_distilbert.csv'  # Path to your merged QA file\n",
        "\n",
        "    print(\"Evaluating QA metrics...\")\n",
        "    metrics = evaluate_qa_metrics(csv_path)\n",
        "\n",
        "    print(\"\\n=== QA Evaluation Results for DISTILBERT ===\")\n",
        "    print(f\"Exact Match: {metrics['exact_match']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
        "    print(f\"Semantic Similarity: {metrics['similarity_score']:.4f}\")\n",
        "    print(f\"Answer Recall: {metrics['answer_recall']:.4f}\")\n",
        "\n",
        "    csv_path = 'merged_qa_phi2.csv'  # Path to your merged QA file\n",
        "\n",
        "    print(\"Evaluating QA metrics...\")\n",
        "    metrics = evaluate_qa_metrics(csv_path)\n",
        "\n",
        "    print(\"\\n=== QA Evaluation Results for PHI-2===\")\n",
        "    print(f\"Exact Match: {metrics['exact_match']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
        "    print(f\"Semantic Similarity: {metrics['similarity_score']:.4f}\")\n",
        "    print(f\"Answer Recall: {metrics['answer_recall']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the following cell to get the IAA\n",
        "##### (Make sure that all the data files are present as required.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nb28AKY37hv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def calculate_iaa_metrics(file_path):\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Ensure the columns are strings and handle NaN values\n",
        "    df['annotator1_answer'] = df['annotator1_answer'].astype(str).fillna('')\n",
        "    df['annotator2_answer'] = df['annotator2_answer'].astype(str).fillna('')\n",
        "\n",
        "    # Compute percentage agreement\n",
        "    df['exact_match'] = df['annotator1_answer'] == df['annotator2_answer']\n",
        "    percentage_agreement = df['exact_match'].mean() * 100\n",
        "\n",
        "    # Compute Cohen's Kappa\n",
        "    kappa_score = cohen_kappa_score(df['annotator1_answer'], df['annotator2_answer'])\n",
        "\n",
        "    return {\n",
        "        'Percentage Agreement': percentage_agreement,\n",
        "        \"Cohen's Kappa\": kappa_score\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "file_path = 'annotated_qsns_final.csv'  # Replace with your actual file path\n",
        "metrics = calculate_iaa_metrics(file_path)\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxpBpYM3QqPv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
